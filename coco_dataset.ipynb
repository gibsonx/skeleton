{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, json, cv2, numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.datasets import CocoDetection\n",
    "import albumentations as A # Library for augmentations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "from utils import collate_fn\n",
    "from engine import train_one_epoch, evaluate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.RandomRotate90(p=1), # Random rotation of an image by 90 degrees zero or more times\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.5, contrast_limit=0.5, brightness_by_max=True, always_apply=False, p=1), # Random change of brightness & contrast\n",
    "        ], p=1)\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'), # More about keypoint formats used in albumentations library read at https://albumentations.ai/docs/getting_started/keypoints_augmentation/\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels']) # Bboxes should have labels, read more here https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "keypoints_classes_ids2names = {0: 'nose', 1: 'left eye', 2: 'right eye'}\n",
    "\n",
    "def visualize(image, bboxes, keypoints, image_original=None, bboxes_original=None, keypoints_original=None):\n",
    "    fontsize = 18\n",
    "\n",
    "    for bbox in bboxes:\n",
    "        start_point = (bbox[0], bbox[1])\n",
    "        end_point = (bbox[2], bbox[3])\n",
    "        image = cv2.rectangle(image.copy(), start_point, end_point, (0,255,0), 2)\n",
    "\n",
    "    for kps in keypoints:\n",
    "        for idx, kp in enumerate(kps):\n",
    "            image = cv2.circle(image.copy(), tuple(kp), 5, (255,0,0), 10)\n",
    "            image = cv2.putText(image.copy(), \" \" + keypoints_classes_ids2names[idx], tuple(kp), cv2.FONT_HERSHEY_SIMPLEX, 2, (255,0,0), 3, cv2.LINE_AA)\n",
    "\n",
    "    if image_original is None and keypoints_original is None:\n",
    "        plt.figure(figsize=(40,40))\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "        print('no keypoint')\n",
    "    else:\n",
    "        print('keypoint is found')\n",
    "        for bbox in bboxes_original:\n",
    "            start_point = (bbox[0], bbox[1])\n",
    "            end_point = (bbox[2], bbox[3])\n",
    "            image_original = cv2.rectangle(image_original.copy(), start_point, end_point, (0,255,0), 2)\n",
    "\n",
    "        for kps in keypoints_original:\n",
    "            for idx, kp in enumerate(kps):\n",
    "                image_original = cv2.circle(image_original, tuple(kp), 5, (255,0,0), 10)\n",
    "                image_original = cv2.putText(image_original, \" \" + keypoints_classes_ids2names[idx], tuple(kp), cv2.FONT_HERSHEY_SIMPLEX, 2, (255,0,0), 3, cv2.LINE_AA)\n",
    "\n",
    "        f, ax = plt.subplots(1, 2, figsize=(40, 20))\n",
    "\n",
    "        ax[0].imshow(image_original)\n",
    "        ax[0].set_title('Original image', fontsize=fontsize)\n",
    "\n",
    "        ax[1].imshow(image)\n",
    "        ax[1].set_title('Transformed image', fontsize=fontsize)\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##########CocoDataset########"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "import os.path\n",
    "from typing import Any, Callable, List, Optional, Tuple\n",
    "from PIL import Image\n",
    "\n",
    "class CocoDataset(CocoDetection):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        annFile: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "        transforms: Optional[Callable] = None,\n",
    "        demo = False\n",
    "    ) -> None:\n",
    "        super().__init__(root, transforms, transform, target_transform)\n",
    "        from pycocotools.coco import COCO\n",
    "\n",
    "        self.coco = COCO(annFile)\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "\n",
    "        self.transform = transform\n",
    "        self.demo = demo # Use demo=True if you need transformed and original images (for example, for visualization purposes)\n",
    "\n",
    "\n",
    "    def _load_image(self, id: int) -> Image.Image:\n",
    "        path = self.coco.loadImgs(id)[0][\"file_name\"]\n",
    "        return Image.open(os.path.join(self.root, path)).convert(\"RGB\")\n",
    "\n",
    "    def _load_target(self, id: int) -> List[Any]:\n",
    "        return self.coco.loadAnns(self.coco.getAnnIds(id))\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        id = self.ids[index]\n",
    "        image = self._load_image(id)\n",
    "        target = self._load_target(id)\n",
    "\n",
    "        img_original = np.array(image)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        bboxes_original = []\n",
    "        for i in target:\n",
    "            bboxes_original.append([ i['bbox'][0], i['bbox'][1], i['bbox'][0] + i['bbox'][2], i['bbox'][1] + i['bbox'][3]])\n",
    "\n",
    "        keypoints_original = [[list(a) for a in zip(*[iter(i['keypoints'])]*3)] for i in target]\n",
    "\n",
    "        bboxes_labels_original = ['ThreePoints' for _ in bboxes_original]\n",
    "\n",
    "        if self.transform:\n",
    "            # Converting keypoints from [x,y,visibility]-format to [x, y]-format + Flattening nested list of keypoints\n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]], where each keypoint is in [x, y]-format\n",
    "            # Then we need to convert it to the following list:\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2]\n",
    "            keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "\n",
    "\n",
    "            # Apply augmentations\n",
    "            transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "            img = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "\n",
    "            # Unflattening list transformed['keypoints']\n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2], where each keypoint is in [x, y]-format\n",
    "            # Then we need to convert it to the following list:\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]]\n",
    "            keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,3,2)).tolist()\n",
    "\n",
    "            # Converting transformed keypoints from [x, y]-format to [x,y,visibility]-format by appending original visibilities to transformed coordinates of keypoints\n",
    "            keypoints = []\n",
    "            for o_idx, obj in enumerate(keypoints_transformed_unflattened): # Iterating over objects\n",
    "                obj_keypoints = []\n",
    "                for k_idx, kp in enumerate(obj): # Iterating over keypoints in each object\n",
    "                    # kp - coordinates of keypoint\n",
    "                    # keypoints_original[o_idx][k_idx][2] - original visibility of keypoint\n",
    "                    obj_keypoints.append(kp + [keypoints_original[o_idx][k_idx][2]])\n",
    "                keypoints.append(obj_keypoints)\n",
    "\n",
    "        else:\n",
    "            img, bboxes, keypoints = img_original, bboxes_original, keypoints_original\n",
    "\n",
    "        # Convert everything into a torch tensor\n",
    "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)\n",
    "        target = {}\n",
    "        target[\"boxes\"] = bboxes\n",
    "        target[\"labels\"] = torch.as_tensor([1 for _ in bboxes], dtype=torch.int64) # all objects are glue tubes\n",
    "        target[\"image_id\"] = torch.tensor([id])\n",
    "        target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "        target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "        img = F.to_tensor(img)\n",
    "\n",
    "        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "        target_original = {}\n",
    "        target_original[\"boxes\"] = bboxes_original\n",
    "        target_original[\"labels\"] = torch.as_tensor([1 for _ in bboxes_original], dtype=torch.int64) # all objects are glue tubes\n",
    "        target_original[\"image_id\"] = torch.tensor([id])\n",
    "        target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "        target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "        target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)\n",
    "        img_original = F.to_tensor(img_original)\n",
    "\n",
    "        if self.demo:\n",
    "            return img, target, img_original, target_original\n",
    "        else:\n",
    "            return img, target\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.ids)\n",
    "\n",
    "        return image, target"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "json_path = r\"C:\\Users\\Administrator\\Desktop\\cats\\annotations\\person_keypoints_Train.json\"\n",
    "img_path = r\"C:\\Users\\Administrator\\Desktop\\cats\\images\"\n",
    "dataset = CocoDataset(annFile=json_path,root=img_path,transform=train_transform(), demo=True)\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "iterator = iter(data_loader)\n",
    "batch = next(iterator)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "image = (batch[0][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes = batch[1][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "keypoints = []\n",
    "for kps in batch[1][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints.append([kp[:2] for kp in kps])\n",
    "\n",
    "image_original = (batch[2][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes_original = batch[3][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "keypoints_original = []\n",
    "for kps in batch[3][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints_original.append([kp[:2] for kp in kps])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoint is found\n"
     ]
    }
   ],
   "source": [
    "visualize(image, bboxes, keypoints, image_original, bboxes_original, keypoints_original)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "def get_model(num_keypoints, weights_path=None):\n",
    "\n",
    "    anchor_generator = AnchorGenerator(sizes=(32, 64, 128, 256, 512), aspect_ratios=(0.25, 0.5, 0.75, 1.0, 2.0, 3.0, 4.0))\n",
    "    model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=False,\n",
    "                                                                   pretrained_backbone=True,\n",
    "                                                                   num_keypoints=num_keypoints,\n",
    "                                                                   num_classes = 2, # Background is the first class, object is the second class\n",
    "                                                                   rpn_anchor_generator=anchor_generator)\n",
    "\n",
    "    if weights_path:\n",
    "        state_dict = torch.load(weights_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with cpu\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Epoch: [0]  [ 0/63]  eta: 0:28:46  lr: 0.000017  loss: 9.5270 (9.5270)  loss_classifier: 0.7113 (0.7113)  loss_box_reg: 0.0089 (0.0089)  loss_keypoint: 8.1067 (8.1067)  loss_objectness: 0.6955 (0.6955)  loss_rpn_box_reg: 0.0045 (0.0045)  time: 27.3980  data: 0.0390\n",
      "Epoch: [0]  [10/63]  eta: 0:26:05  lr: 0.000178  loss: 9.4254 (9.4313)  loss_classifier: 0.6929 (0.6776)  loss_box_reg: 0.0089 (0.0100)  loss_keypoint: 8.0311 (8.0420)  loss_objectness: 0.6950 (0.6949)  loss_rpn_box_reg: 0.0055 (0.0068)  time: 29.5294  data: 0.0670\n",
      "Epoch: [0]  [20/63]  eta: 0:21:22  lr: 0.000339  loss: 9.2375 (9.1869)  loss_classifier: 0.5762 (0.5609)  loss_box_reg: 0.0081 (0.0106)  loss_keypoint: 7.9656 (7.9147)  loss_objectness: 0.6936 (0.6933)  loss_rpn_box_reg: 0.0068 (0.0073)  time: 29.9576  data: 0.0649\n",
      "Epoch: [0]  [30/63]  eta: 0:17:17  lr: 0.000501  loss: 8.6237 (8.9044)  loss_classifier: 0.2310 (0.4252)  loss_box_reg: 0.0087 (0.0131)  loss_keypoint: 7.6799 (7.7707)  loss_objectness: 0.6878 (0.6884)  loss_rpn_box_reg: 0.0062 (0.0071)  time: 32.5013  data: 0.0684\n",
      "Epoch: [0]  [40/63]  eta: 0:12:11  lr: 0.000662  loss: 8.1125 (8.6838)  loss_classifier: 0.0905 (0.3384)  loss_box_reg: 0.0081 (0.0117)  loss_keypoint: 7.3454 (7.6516)  loss_objectness: 0.6585 (0.6747)  loss_rpn_box_reg: 0.0062 (0.0074)  time: 33.8712  data: 0.0722\n",
      "Epoch: [0]  [50/63]  eta: 0:06:52  lr: 0.000823  loss: 7.4725 (8.3953)  loss_classifier: 0.0447 (0.2796)  loss_box_reg: 0.0055 (0.0110)  loss_keypoint: 6.7773 (7.4732)  loss_objectness: 0.5194 (0.6238)  loss_rpn_box_reg: 0.0073 (0.0078)  time: 32.1362  data: 0.0563\n",
      "Epoch: [0]  [60/63]  eta: 0:01:35  lr: 0.000984  loss: 7.3719 (8.1983)  loss_classifier: 0.0378 (0.2411)  loss_box_reg: 0.0066 (0.0111)  loss_keypoint: 7.0317 (7.3810)  loss_objectness: 0.2946 (0.5572)  loss_rpn_box_reg: 0.0074 (0.0079)  time: 32.1683  data: 0.0510\n",
      "Epoch: [0]  [62/63]  eta: 0:00:31  lr: 0.001000  loss: 7.4430 (8.1727)  loss_classifier: 0.0390 (0.2348)  loss_box_reg: 0.0074 (0.0112)  loss_keypoint: 7.0963 (7.3741)  loss_objectness: 0.2697 (0.5446)  loss_rpn_box_reg: 0.0074 (0.0079)  time: 31.8866  data: 0.0519\n",
      "Epoch: [0] Total time: 0:33:23 (31.7985 s / it)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[70], line 27\u001B[0m\n\u001B[0;32m     25\u001B[0m     train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n\u001B[0;32m     26\u001B[0m     lr_scheduler\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m---> 27\u001B[0m     \u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_loader_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;66;03m# Save model weights after training\u001B[39;00m\n\u001B[0;32m     31\u001B[0m torch\u001B[38;5;241m.\u001B[39msave(model\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mC:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mUsers\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mAdministrator\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mPycharmProjects\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mskeleton\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mout\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mkeypointsrcnn_weights.pth\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\skeleton\\venv\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001B[0m, in \u001B[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     26\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclone():\n\u001B[1;32m---> 27\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\skeleton\\engine.py:83\u001B[0m, in \u001B[0;36mevaluate\u001B[1;34m(model, data_loader, device)\u001B[0m\n\u001B[0;32m     81\u001B[0m coco \u001B[38;5;241m=\u001B[39m get_coco_api_from_dataset(data_loader\u001B[38;5;241m.\u001B[39mdataset)\n\u001B[0;32m     82\u001B[0m iou_types \u001B[38;5;241m=\u001B[39m _get_iou_types(model)\n\u001B[1;32m---> 83\u001B[0m coco_evaluator \u001B[38;5;241m=\u001B[39m \u001B[43mCocoEvaluator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcoco\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miou_types\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     85\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m images, targets \u001B[38;5;129;01min\u001B[39;00m metric_logger\u001B[38;5;241m.\u001B[39mlog_every(data_loader, \u001B[38;5;241m100\u001B[39m, header):\n\u001B[0;32m     86\u001B[0m     images \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(img\u001B[38;5;241m.\u001B[39mto(device) \u001B[38;5;28;01mfor\u001B[39;00m img \u001B[38;5;129;01min\u001B[39;00m images)\n",
      "File \u001B[1;32m~\\PycharmProjects\\skeleton\\coco_eval.py:22\u001B[0m, in \u001B[0;36mCocoEvaluator.__init__\u001B[1;34m(self, coco_gt, iou_types)\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcoco_eval \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m iou_type \u001B[38;5;129;01min\u001B[39;00m iou_types:\n\u001B[1;32m---> 22\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcoco_eval[iou_type] \u001B[38;5;241m=\u001B[39m \u001B[43mCOCOeval\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcoco_gt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miouType\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43miou_type\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimg_ids \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39meval_imgs \u001B[38;5;241m=\u001B[39m {k: [] \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m iou_types}\n",
      "File \u001B[1;32m~\\PycharmProjects\\skeleton\\venv\\lib\\site-packages\\pycocotools\\cocoeval.py:76\u001B[0m, in \u001B[0;36mCOCOeval.__init__\u001B[1;34m(self, cocoGt, cocoDt, iouType)\u001B[0m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gts \u001B[38;5;241m=\u001B[39m defaultdict(\u001B[38;5;28mlist\u001B[39m)       \u001B[38;5;66;03m# gt for evaluation\u001B[39;00m\n\u001B[0;32m     75\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dts \u001B[38;5;241m=\u001B[39m defaultdict(\u001B[38;5;28mlist\u001B[39m)       \u001B[38;5;66;03m# dt for evaluation\u001B[39;00m\n\u001B[1;32m---> 76\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams \u001B[38;5;241m=\u001B[39m \u001B[43mParams\u001B[49m\u001B[43m(\u001B[49m\u001B[43miouType\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43miouType\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# parameters\u001B[39;00m\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_paramsEval \u001B[38;5;241m=\u001B[39m {}               \u001B[38;5;66;03m# parameters for evaluation\u001B[39;00m\n\u001B[0;32m     78\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstats \u001B[38;5;241m=\u001B[39m []                     \u001B[38;5;66;03m# result summarization\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\skeleton\\venv\\lib\\site-packages\\pycocotools\\cocoeval.py:527\u001B[0m, in \u001B[0;36mParams.__init__\u001B[1;34m(self, iouType)\u001B[0m\n\u001B[0;32m    525\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, iouType\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msegm\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m    526\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m iouType \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msegm\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m iouType \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbbox\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m--> 527\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msetDetParams\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    528\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m iouType \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mkeypoints\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    529\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msetKpParams()\n",
      "File \u001B[1;32m~\\PycharmProjects\\skeleton\\venv\\lib\\site-packages\\pycocotools\\cocoeval.py:507\u001B[0m, in \u001B[0;36mParams.setDetParams\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    505\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcatIds \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    506\u001B[0m \u001B[38;5;66;03m# np.arange causes trouble.  the data point on arange is slightly larger than the true value\u001B[39;00m\n\u001B[1;32m--> 507\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miouThrs \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinspace\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m.5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.95\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mround\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.95\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m.5\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m.05\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mendpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    508\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrecThrs \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mlinspace(\u001B[38;5;241m.0\u001B[39m, \u001B[38;5;241m1.00\u001B[39m, np\u001B[38;5;241m.\u001B[39mround((\u001B[38;5;241m1.00\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m.0\u001B[39m) \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m.01\u001B[39m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m, endpoint\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    509\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmaxDets \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m100\u001B[39m]\n",
      "File \u001B[1;32m<__array_function__ internals>:200\u001B[0m, in \u001B[0;36mlinspace\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[1;32m~\\PycharmProjects\\skeleton\\venv\\lib\\site-packages\\numpy\\core\\function_base.py:121\u001B[0m, in \u001B[0;36mlinspace\u001B[1;34m(start, stop, num, endpoint, retstep, dtype, axis)\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;129m@array_function_dispatch\u001B[39m(_linspace_dispatcher)\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlinspace\u001B[39m(start, stop, num\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m, endpoint\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, retstep\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     25\u001B[0m              axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m):\n\u001B[0;32m     26\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;124;03m    Return evenly spaced numbers over a specified interval.\u001B[39;00m\n\u001B[0;32m     28\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    119\u001B[0m \n\u001B[0;32m    120\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 121\u001B[0m     num \u001B[38;5;241m=\u001B[39m \u001B[43moperator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    122\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m num \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    123\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNumber of samples, \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m, must be non-negative.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m num)\n",
      "\u001B[1;31mTypeError\u001B[0m: 'numpy.float64' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('training with cpu')\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "KEYPOINTS_FOLDER_TRAIN = r\"C:\\Users\\Administrator\\Desktop\\cats\\annotations\\person_keypoints_Train.json\"\n",
    "KEYPOINTS_FOLDER_TEST = r\"C:\\Users\\Administrator\\Desktop\\cats\\annotations\\person_keypoints_Validation.json\"\n",
    "\n",
    "dataset_train = CocoDataset(annFile=KEYPOINTS_FOLDER_TRAIN,root=img_path,transform=train_transform(), demo=False)\n",
    "dataset_test = CocoDataset(annFile=KEYPOINTS_FOLDER_TEST,root=img_path, transform=None, demo=False)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=3, shuffle=True, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model = get_model(num_keypoints = 3)\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=10)\n",
    "    lr_scheduler.step()\n",
    "    evaluate(model, data_loader_test, device)\n",
    "\n",
    "\n",
    "# Save model weights after training\n",
    "torch.save(model.state_dict(), r'C:\\Users\\Administrator\\PycharmProjects\\skeleton\\out\\keypointsrcnn_weights.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "iterator = iter(data_loader_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "images, targets = next(iterator)\n",
    "images = list(image.to(device) for image in images)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    output = model(images)\n",
    "\n",
    "print(\"Predictions: \\n\", output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image = (images[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "scores = output[0]['scores'].detach().cpu().numpy()\n",
    "\n",
    "high_scores_idxs = np.where(scores > 0.5)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "# Below, in output[0]['keypoints'][high_scores_idxs][post_nms_idxs] and output[0]['boxes'][high_scores_idxs][post_nms_idxs]\n",
    "# Firstly, we choose only those objects, which have score above predefined threshold. This is done with choosing elements with [high_scores_idxs] indexes\n",
    "# Secondly, we choose only those objects, which are left after NMS is applied. This is done with choosing elements with [post_nms_idxs] indexes\n",
    "\n",
    "keypoints = []\n",
    "for kps in output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "\n",
    "bboxes = []\n",
    "for bbox in output[0]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    bboxes.append(list(map(int, bbox.tolist())))\n",
    "\n",
    "visualize(image, bboxes, keypoints)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}