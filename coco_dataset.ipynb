{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, json, cv2, numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.datasets import CocoDetection\n",
    "import albumentations as A # Library for augmentations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from utils import collate_fn\n",
    "from engine import train_one_epoch, evaluate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "{0: '0',\n 1: '1',\n 2: '2',\n 3: '3',\n 4: '4',\n 5: '5',\n 6: '6',\n 7: '7',\n 8: '8',\n 9: '9',\n 10: '10',\n 11: '11',\n 12: '12',\n 13: '13',\n 14: '14',\n 15: '15',\n 16: '16',\n 17: '17',\n 18: '18',\n 19: '19',\n 20: '20',\n 21: '21',\n 22: '22',\n 23: '23',\n 24: '24',\n 25: '25',\n 26: '26',\n 27: '27',\n 28: '28',\n 29: '29',\n 30: '30',\n 31: '31',\n 32: '32',\n 33: '33',\n 34: '34',\n 35: '35',\n 36: '36',\n 37: '37',\n 38: '38',\n 39: '39',\n 40: '40',\n 41: '41',\n 42: '42',\n 43: '43',\n 44: '44',\n 45: '45',\n 46: '46',\n 47: '47',\n 48: '48',\n 49: '49',\n 50: '50',\n 51: '51',\n 52: '52',\n 53: '53',\n 54: '54',\n 55: '55',\n 56: '56',\n 57: '57',\n 58: '58',\n 59: '59',\n 60: '60',\n 61: '61',\n 62: '62',\n 63: '63',\n 64: '64',\n 65: '65',\n 66: '66',\n 67: '67',\n 68: '68',\n 69: '69',\n 70: '70',\n 71: '71',\n 72: '72',\n 73: '73',\n 74: '74',\n 75: '75',\n 76: '76',\n 77: '77',\n 78: '78',\n 79: '79',\n 80: '80',\n 81: '81',\n 82: '82',\n 83: '83',\n 84: '84',\n 85: '85',\n 86: '86',\n 87: '87',\n 88: '88',\n 89: '89',\n 90: '90',\n 91: '91',\n 92: '92',\n 93: '93',\n 94: '94',\n 95: '95',\n 96: '96',\n 97: '97',\n 98: '98',\n 99: '99',\n 100: '100',\n 101: '101',\n 102: '102',\n 103: '103',\n 104: '104',\n 105: '105',\n 106: '106',\n 107: '107',\n 108: '108',\n 109: '109',\n 110: '110',\n 111: '111',\n 112: '112',\n 113: '113',\n 114: '114',\n 115: '115',\n 116: '116',\n 117: '117',\n 118: '118',\n 119: '119',\n 120: '120',\n 121: '121',\n 122: '122',\n 123: '123',\n 124: '124',\n 125: '125',\n 126: '126',\n 127: '127',\n 128: '128',\n 129: '129',\n 130: '130',\n 131: '131',\n 132: '132',\n 133: '133',\n 134: '134',\n 135: '135',\n 136: '136'}"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = 0\n",
    "dict = {}\n",
    "for i in range(137):\n",
    "    dict[e] = str(e)\n",
    "    e+=1\n",
    "dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            # A.RandomRotate90(p=1), # Random rotation of an image by 90 degrees zero or more times\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, brightness_by_max=True, always_apply=False, p=1), # Random change of brightness & contrast\n",
    "        ], p=1)\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'), # More about keypoint formats used in albumentations library read at https://albumentations.ai/docs/getting_started/keypoints_augmentation/\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels']) # Bboxes should have labels, read more here https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "keypoints_classes_ids2names = {0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9', 10: '10', 11: '11', 12: '12', 13: '13', 14: '14', 15: '15', 16: '16', 17: '17', 18: '18', 19: '19', 20: '20', 21: '21', 22: '22', 23: '23', 24: '24', 25: '25', 26: '26', 27: '27', 28: '28', 29: '29', 30: '30', 31: '31', 32: '32', 33: '33', 34: '34', 35: '35', 36: '36', 37: '37', 38: '38', 39: '39', 40: '40', 41: '41', 42: '42', 43: '43', 44: '44', 45: '45', 46: '46', 47: '47', 48: '48', 49: '49', 50: '50', 51: '51', 52: '52', 53: '53', 54: '54', 55: '55', 56: '56', 57: '57', 58: '58', 59: '59', 60: '60', 61: '61', 62: '62', 63: '63', 64: '64', 65: '65', 66: '66', 67: '67', 68: '68', 69: '69', 70: '70', 71: '71', 72: '72', 73: '73', 74: '74', 75: '75', 76: '76', 77: '77', 78: '78', 79: '79', 80: '80', 81: '81', 82: '82', 83: '83', 84: '84', 85: '85', 86: '86', 87: '87', 88: '88', 89: '89', 90: '90', 91: '91', 92: '92', 93: '93', 94: '94', 95: '95', 96: '96', 97: '97', 98: '98', 99: '99', 100: '100', 101: '101', 102: '102', 103: '103', 104: '104', 105: '105', 106: '106', 107: '107', 108: '108', 109: '109', 110: '110', 111: '111', 112: '112', 113: '113', 114: '114', 115: '115', 116: '116', 117: '117', 118: '118', 119: '119', 120: '120', 121: '121', 122: '122', 123: '123', 124: '124', 125: '125', 126: '126', 127: '127', 128: '128', 129: '129', 130: '130', 131: '131', 132: '132', 133: '133', 134: '134', 135: '135', 136: '136'}\n",
    "\n",
    "def visualize(image, bboxes, keypoints, image_original=None, bboxes_original=None, keypoints_original=None):\n",
    "    fontsize = 2\n",
    "\n",
    "    for bbox in bboxes:\n",
    "        start_point = (bbox[0], bbox[1])\n",
    "        end_point = (bbox[2], bbox[3])\n",
    "        image = cv2.rectangle(image.copy(), start_point, end_point, (0,255,0), 2)\n",
    "\n",
    "    for kps in keypoints:\n",
    "        for idx, kp in enumerate(kps):\n",
    "            image = cv2.circle(image.copy(), tuple(kp), 5, (255,0,0), 10)\n",
    "            # image = cv2.putText(image.copy(), \" \" + keypoints_classes_ids2names[idx], tuple(kp), cv2.FONT_HERSHEY_SIMPLEX, 2, (255,0,0), 3, cv2.LINE_AA)\n",
    "\n",
    "    if image_original is None and keypoints_original is None:\n",
    "        plt.figure(figsize=(40,40))\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "        print('no keypoint')\n",
    "    else:\n",
    "        print('keypoint is found')\n",
    "        for bbox in bboxes_original:\n",
    "            start_point = (bbox[0], bbox[1])\n",
    "            end_point = (bbox[2], bbox[3])\n",
    "            image_original = cv2.rectangle(image_original.copy(), start_point, end_point, (0,255,0), 2)\n",
    "\n",
    "        for kps in keypoints_original:\n",
    "            for idx, kp in enumerate(kps):\n",
    "                image_original = cv2.circle(image_original, tuple(kp), 5, (255,0,0), 10)\n",
    "                image_original = cv2.putText(image_original, \" \" + keypoints_classes_ids2names[idx], tuple(kp), cv2.FONT_HERSHEY_SIMPLEX, 2, (255,0,0), 3, cv2.LINE_AA)\n",
    "\n",
    "        f, ax = plt.subplots(1, 2, figsize=(40, 20))\n",
    "\n",
    "        ax[0].imshow(image_original)\n",
    "        ax[0].set_title('Original image', fontsize=fontsize)\n",
    "\n",
    "        ax[1].imshow(image)\n",
    "        ax[1].set_title('Transformed image', fontsize=fontsize)\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##########CocoDataset########"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "import os.path\n",
    "from typing import Any, Callable, List, Optional, Tuple\n",
    "from PIL import Image\n",
    "\n",
    "class CocoDataset(CocoDetection):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        annFile: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "        transforms: Optional[Callable] = None,\n",
    "        demo = False\n",
    "    ) -> None:\n",
    "        super().__init__(root, transforms, transform, target_transform)\n",
    "        from pycocotools.coco import COCO\n",
    "\n",
    "        self.coco = COCO(annFile)\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "\n",
    "        self.transform = transform\n",
    "        self.demo = demo # Use demo=True if you need transformed and original images (for example, for visualization purposes)\n",
    "\n",
    "\n",
    "    def _load_image(self, id: int) -> Image.Image:\n",
    "        path = self.coco.loadImgs(id)[0][\"file_name\"]\n",
    "        return Image.open(os.path.join(self.root, path)).convert(\"RGB\")\n",
    "\n",
    "    def _load_target(self, id: int) -> List[Any]:\n",
    "        return self.coco.loadAnns(self.coco.getAnnIds(id))\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        id = self.ids[index]\n",
    "        image = self._load_image(id)\n",
    "        target = self._load_target(id)\n",
    "\n",
    "        img_original = np.array(image)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        bboxes_original = []\n",
    "        for i in target:\n",
    "            print(i['bbox'])\n",
    "            bboxes_original.append([ i['bbox'][0], i['bbox'][1], i['bbox'][0] + i['bbox'][2], i['bbox'][1] + i['bbox'][3]])\n",
    "\n",
    "\n",
    "        keypoints_original = [[list(a) for a in zip(*[iter(i['keypoints'])]*3)] for i in target]\n",
    "\n",
    "        bboxes_labels_original = ['137points' for _ in bboxes_original]\n",
    "\n",
    "        if self.transform:\n",
    "            # Converting keypoints from [x,y,visibility]-format to [x, y]-format + Flattening nested list of keypoints\n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]], where each keypoint is in [x, y]-format\n",
    "            # Then we need to convert it to the following list:\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2]\n",
    "            keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "\n",
    "\n",
    "            # Apply augmentations\n",
    "            transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "            img = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "\n",
    "            # Unflattening list transformed['keypoints']\n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2], where each keypoint is in [x, y]-format\n",
    "            # Then we need to convert it to the following list:\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]]\n",
    "            keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,137,2)).tolist()\n",
    "\n",
    "            # Converting transformed keypoints from [x, y]-format to [x,y,visibility]-format by appending original visibilities to transformed coordinates of keypoints\n",
    "            keypoints = []\n",
    "            for o_idx, obj in enumerate(keypoints_transformed_unflattened): # Iterating over objects\n",
    "                obj_keypoints = []\n",
    "                for k_idx, kp in enumerate(obj): # Iterating over keypoints in each object\n",
    "                    # kp - coordinates of keypoint\n",
    "                    # keypoints_original[o_idx][k_idx][2] - original visibility of keypoint\n",
    "                    obj_keypoints.append(kp + [keypoints_original[o_idx][k_idx][2]])\n",
    "                keypoints.append(obj_keypoints)\n",
    "\n",
    "        else:\n",
    "            img, bboxes, keypoints = img_original, bboxes_original, keypoints_original\n",
    "\n",
    "        # Convert everything into a torch tensor\n",
    "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)\n",
    "        target = {}\n",
    "        target[\"boxes\"] = bboxes\n",
    "        target[\"labels\"] = torch.as_tensor([1 for _ in bboxes], dtype=torch.int64) # all objects are glue tubes\n",
    "        target[\"image_id\"] = torch.tensor([id])\n",
    "        # target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "        target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "        img = F.to_tensor(img)\n",
    "\n",
    "        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "        target_original = {}\n",
    "        target_original[\"boxes\"] = bboxes_original\n",
    "        target_original[\"labels\"] = torch.as_tensor([1 for _ in bboxes_original], dtype=torch.int64) # all objects are glue tubes\n",
    "        target_original[\"image_id\"] = torch.tensor([id])\n",
    "        # target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "        target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "        target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)\n",
    "        img_original = F.to_tensor(img_original)\n",
    "\n",
    "        if self.demo:\n",
    "            return img, target, img_original, target_original\n",
    "        else:\n",
    "            return img, target\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.ids)\n",
    "\n",
    "        return image, target"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "[197.5, -8.72, 709.04, 2101.83]\n",
      "[147.33, -44.7, 816.83, 2096.47]\n"
     ]
    }
   ],
   "source": [
    "json_path = r\"C:\\Users\\Administrator\\Downloads\\sk0413\\annotations\\person_keypoints_default.json\"\n",
    "img_path = r\"C:\\Users\\Administrator\\Downloads\\sk0413\\images\"\n",
    "dataset = CocoDataset(annFile=json_path,root=img_path,transform=None, demo=True)\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "iterator = iter(data_loader)\n",
    "batch = next(iterator)\n",
    "batch = next(iterator)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "image = (batch[0][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes = batch[1][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "keypoints = []\n",
    "for kps in batch[1][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints.append([kp[:2] for kp in kps])\n",
    "\n",
    "image_original = (batch[2][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes_original = batch[3][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "keypoints_original = []\n",
    "for kps in batch[3][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints_original.append([kp[:2] for kp in kps])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoint is found\n"
     ]
    }
   ],
   "source": [
    "visualize(image, bboxes, keypoints, image_original, bboxes_original, keypoints_original)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "def get_model(num_keypoints, weights_path=None):\n",
    "\n",
    "    anchor_generator = AnchorGenerator(sizes=(32, 64, 128, 256, 512), aspect_ratios=(0.25, 0.5, 0.75, 1.0, 2.0, 3.0, 4.0))\n",
    "    model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=False,\n",
    "                                                                   pretrained_backbone=True,\n",
    "                                                                   num_keypoints=num_keypoints,\n",
    "                                                                   num_classes = 2, # Background is the first class, object is the second class\n",
    "                                                                   rpn_anchor_generator=anchor_generator)\n",
    "\n",
    "    if weights_path:\n",
    "        state_dict = torch.load(weights_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with cpu\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "[146.98, 18.71, 796.27, 2004.04]\n",
      "[160.93, 290.81, 693.57, 1864.47]\n",
      "[181.38, 197.57, 733.91, 1813.89]\n",
      "[186.81, 16.11, 758.02, 2012.36]\n",
      "[185.04, 15.28, 730.93, 2015.92]\n",
      "[239.31, 109.32, 672.63, 1904.35]\n",
      "[114.9, -30.6, 857.57, 2082.25]\n",
      "[157.55, 16.38, 792.04, 2016.9]\n",
      "[195.02, 31.76, 751.64, 2059.88]\n",
      "[232.28, 213.96, 660.17, 1714.34]\n",
      "Epoch: [0]  [0/7]  eta: 0:21:07  lr: 0.000168  loss: 9.4448 (9.4448)  loss_classifier: 0.6375 (0.6375)  loss_box_reg: 0.0040 (0.0040)  loss_keypoint: 8.0548 (8.0548)  loss_objectness: 0.6938 (0.6938)  loss_rpn_box_reg: 0.0547 (0.0547)  time: 181.1050  data: 0.8175\n",
      "[142.61, 21.95, 780.73, 2022.7]\n",
      "[144.63, -22.11, 815.85, 1995.38]\n",
      "[219.64, 124.89, 770.69, 1870.89]\n",
      "[194.26, 27.37, 713.76, 1986.82]\n",
      "[166.02, 27.11, 776.51, 1961.81]\n",
      "[193.52, 23.69, 753.59, 2034.33]\n",
      "[278.3, 189.58, 594.08, 1730.92]\n",
      "[198.45, 49.22, 773.64, 1934.19]\n",
      "[225.57, 99.81, 701.13, 2040.71]\n",
      "[183.88, 22.16, 773.79, 2001.75]\n",
      "[147.33, -44.7, 816.83, 2096.47]\n",
      "[151.7, -54.29, 756.47, 2088.73]\n",
      "[174.66, -0.56, 778.4, 2045.82]\n",
      "[191.24, 9.85, 705.15, 2061.1]\n",
      "[350.48, -25.34, 546.35, 1956.47]\n",
      "[192.46, 4.57, 752.69, 2032.27]\n",
      "[193.15, -15.13, 675.13, 2062.17]\n",
      "[182.78, 111.61, 724.75, 1897.97]\n",
      "[168.47, 13.46, 767.34, 2044.54]\n",
      "[191.66, -19.87, 756.85, 2049.47]\n",
      "[288.52, 59.7, 588.82, 2027.28]\n",
      "[246.02, 76.79, 645.68, 1796.36]\n",
      "[134.08, 7.25, 769.52, 2078.11]\n",
      "[220.01, 108.22, 702.37, 1864.66]\n",
      "[194.17, -12.83, 709.7, 2113.05]\n",
      "[169.85, 1.56, 747.4, 2058.47]\n",
      "[180.15, -51.21, 756.17, 2122.49]\n",
      "[173.3, 18.4, 740.54, 2020.4]\n",
      "[195.43, 76.37, 715.05, 1961.72]\n",
      "[166.01, -43.0, 797.55, 2099.59]\n",
      "[193.51, 16.0, 756.04, 1989.76]\n",
      "[143.89, -1.03, 814.78, 2109.61]\n",
      "[187.77, -26.23, 698.18, 2001.71]\n",
      "[155.8, -0.73, 818.09, 2048.1]\n",
      "[171.15, 3.48, 798.22, 2078.97]\n",
      "[306.56, -51.54, 634.61, 2100.03]\n",
      "[238.22, 138.98, 704.38, 2000.23]\n",
      "[165.7, 68.62, 817.03, 1973.51]\n",
      "[184.34, 76.23, 709.9, 1976.6]\n",
      "[168.72, -26.5, 774.65, 2057.63]\n",
      "[196.08, -56.5, 703.98, 2089.99]\n",
      "[127.57, -7.18, 809.14, 2067.02]\n",
      "[243.17, 134.41, 664.01, 1753.4]\n",
      "[167.33, -0.29, 763.0, 2049.1]\n",
      "[257.25, 43.54, 684.49, 1993.59]\n",
      "[181.46, 44.23, 754.87, 1997.44]\n",
      "[235.71, 230.24, 647.6, 1684.62]\n",
      "[85.75, 0.4, 910.32, 2200.31]\n",
      "[258.52, 142.53, 672.39, 1865.19]\n",
      "[187.41, -5.43, 719.19, 2062.01]\n",
      "[197.28, -1.69, 721.55, 2050.66]\n",
      "[150.83, 0.72, 823.69, 2053.08]\n",
      "[197.5, -8.72, 709.04, 2101.83]\n",
      "[190.34, 158.9, 668.45, 1720.88]\n",
      "[202.5, 9.26, 714.34, 2053.56]\n",
      "[144.66, -219.05, 862.31, 2303.9]\n",
      "[212.43, 58.96, 732.99, 1958.58]\n",
      "[214.96, 54.33, 700.32, 1946.17]\n",
      "[152.68, 5.3, 733.76, 2042.59]\n",
      "[170.42, 9.5, 782.99, 2070.8]\n",
      "Epoch: [0]  [6/7]  eta: 0:02:56  lr: 0.001000  loss: 9.3904 (9.3531)  loss_classifier: 0.5828 (0.5385)  loss_box_reg: 0.0041 (0.0044)  loss_keypoint: 8.0567 (8.0572)  loss_objectness: 0.6930 (0.6927)  loss_rpn_box_reg: 0.0598 (0.0604)  time: 176.7882  data: 0.7808\n",
      "Epoch: [0] Total time: 0:20:37 (176.7889 s / it)\n",
      "[196.08, -56.5, 703.98, 2089.99]\n",
      "[142.61, 21.95, 780.73, 2022.7]\n",
      "[350.48, -25.34, 546.35, 1956.47]\n",
      "[165.7, 68.62, 817.03, 1973.51]\n",
      "[195.43, 76.37, 715.05, 1961.72]\n",
      "[168.47, 13.46, 767.34, 2044.54]\n",
      "[180.15, -51.21, 756.17, 2122.49]\n",
      "[306.56, -51.54, 634.61, 2100.03]\n",
      "[184.34, 76.23, 709.9, 1976.6]\n",
      "[190.34, 158.9, 668.45, 1720.88]\n",
      "Epoch: [1]  [0/7]  eta: 0:20:35  lr: 0.001000  loss: 9.0223 (9.0223)  loss_classifier: 0.2329 (0.2329)  loss_box_reg: 0.0104 (0.0104)  loss_keypoint: 8.0519 (8.0519)  loss_objectness: 0.6883 (0.6883)  loss_rpn_box_reg: 0.0387 (0.0387)  time: 176.4288  data: 0.7915\n",
      "[214.96, 54.33, 700.32, 1946.17]\n",
      "[278.3, 189.58, 594.08, 1730.92]\n",
      "[232.28, 213.96, 660.17, 1714.34]\n",
      "[147.33, -44.7, 816.83, 2096.47]\n",
      "[155.8, -0.73, 818.09, 2048.1]\n",
      "[197.28, -1.69, 721.55, 2050.66]\n",
      "[166.01, -43.0, 797.55, 2099.59]\n",
      "[134.08, 7.25, 769.52, 2078.11]\n",
      "[193.51, 16.0, 756.04, 1989.76]\n",
      "[212.43, 58.96, 732.99, 1958.58]\n",
      "[220.01, 108.22, 702.37, 1864.66]\n",
      "[238.22, 138.98, 704.38, 2000.23]\n",
      "[288.52, 59.7, 588.82, 2027.28]\n",
      "[146.98, 18.71, 796.27, 2004.04]\n",
      "[202.5, 9.26, 714.34, 2053.56]\n",
      "[168.72, -26.5, 774.65, 2057.63]\n",
      "[192.46, 4.57, 752.69, 2032.27]\n",
      "[219.64, 124.89, 770.69, 1870.89]\n",
      "[181.38, 197.57, 733.91, 1813.89]\n",
      "[193.15, -15.13, 675.13, 2062.17]\n",
      "[85.75, 0.4, 910.32, 2200.31]\n",
      "[152.68, 5.3, 733.76, 2042.59]\n",
      "[239.31, 109.32, 672.63, 1904.35]\n",
      "[191.24, 9.85, 705.15, 2061.1]\n",
      "[171.15, 3.48, 798.22, 2078.97]\n",
      "[144.66, -219.05, 862.31, 2303.9]\n",
      "[151.7, -54.29, 756.47, 2088.73]\n",
      "[173.3, 18.4, 740.54, 2020.4]\n",
      "[144.63, -22.11, 815.85, 1995.38]\n",
      "[182.78, 111.61, 724.75, 1897.97]\n",
      "[191.66, -19.87, 756.85, 2049.47]\n",
      "[157.55, 16.38, 792.04, 2016.9]\n",
      "[167.33, -0.29, 763.0, 2049.1]\n",
      "[114.9, -30.6, 857.57, 2082.25]\n",
      "[194.26, 27.37, 713.76, 1986.82]\n",
      "[187.77, -26.23, 698.18, 2001.71]\n",
      "[166.02, 27.11, 776.51, 1961.81]\n",
      "[187.41, -5.43, 719.19, 2062.01]\n",
      "[143.89, -1.03, 814.78, 2109.61]\n",
      "[198.45, 49.22, 773.64, 1934.19]\n",
      "[150.83, 0.72, 823.69, 2053.08]\n",
      "[197.5, -8.72, 709.04, 2101.83]\n",
      "[169.85, 1.56, 747.4, 2058.47]\n",
      "[183.88, 22.16, 773.79, 2001.75]\n",
      "[243.17, 134.41, 664.01, 1753.4]\n",
      "[186.81, 16.11, 758.02, 2012.36]\n",
      "[170.42, 9.5, 782.99, 2070.8]\n",
      "[181.46, 44.23, 754.87, 1997.44]\n",
      "[257.25, 43.54, 684.49, 1993.59]\n",
      "[160.93, 290.81, 693.57, 1864.47]\n",
      "[258.52, 142.53, 672.39, 1865.19]\n",
      "[174.66, -0.56, 778.4, 2045.82]\n",
      "[225.57, 99.81, 701.13, 2040.71]\n",
      "[194.17, -12.83, 709.7, 2113.05]\n",
      "[127.57, -7.18, 809.14, 2067.02]\n",
      "[193.52, 23.69, 753.59, 2034.33]\n",
      "[195.02, 31.76, 751.64, 2059.88]\n",
      "[235.71, 230.24, 647.6, 1684.62]\n",
      "[246.02, 76.79, 645.68, 1796.36]\n",
      "[185.04, 15.28, 730.93, 2015.92]\n",
      "Epoch: [1]  [6/7]  eta: 0:02:58  lr: 0.001000  loss: 8.8875 (8.8776)  loss_classifier: 0.0887 (0.1076)  loss_box_reg: 0.0112 (0.0159)  loss_keypoint: 8.0283 (8.0303)  loss_objectness: 0.6787 (0.6766)  loss_rpn_box_reg: 0.0439 (0.0472)  time: 178.5511  data: 0.7787\n",
      "Epoch: [1] Total time: 0:20:49 (178.5514 s / it)\n",
      "[278.3, 189.58, 594.08, 1730.92]\n",
      "[168.72, -26.5, 774.65, 2057.63]\n",
      "[193.51, 16.0, 756.04, 1989.76]\n",
      "[246.02, 76.79, 645.68, 1796.36]\n",
      "[167.33, -0.29, 763.0, 2049.1]\n",
      "[214.96, 54.33, 700.32, 1946.17]\n",
      "[187.41, -5.43, 719.19, 2062.01]\n",
      "[155.8, -0.73, 818.09, 2048.1]\n",
      "[173.3, 18.4, 740.54, 2020.4]\n",
      "[243.17, 134.41, 664.01, 1753.4]\n",
      "Epoch: [2]  [0/7]  eta: 0:21:58  lr: 0.001000  loss: 8.7646 (8.7646)  loss_classifier: 0.0451 (0.0451)  loss_box_reg: 0.0454 (0.0454)  loss_keypoint: 8.0091 (8.0091)  loss_objectness: 0.6487 (0.6487)  loss_rpn_box_reg: 0.0164 (0.0164)  time: 188.3456  data: 0.7925\n",
      "[195.43, 76.37, 715.05, 1961.72]\n",
      "[194.26, 27.37, 713.76, 1986.82]\n",
      "[288.52, 59.7, 588.82, 2027.28]\n",
      "[239.31, 109.32, 672.63, 1904.35]\n",
      "[114.9, -30.6, 857.57, 2082.25]\n",
      "[127.57, -7.18, 809.14, 2067.02]\n",
      "[219.64, 124.89, 770.69, 1870.89]\n",
      "[187.77, -26.23, 698.18, 2001.71]\n",
      "[181.46, 44.23, 754.87, 1997.44]\n",
      "[183.88, 22.16, 773.79, 2001.75]\n",
      "[170.42, 9.5, 782.99, 2070.8]\n",
      "[193.15, -15.13, 675.13, 2062.17]\n",
      "[146.98, 18.71, 796.27, 2004.04]\n",
      "[232.28, 213.96, 660.17, 1714.34]\n",
      "[191.24, 9.85, 705.15, 2061.1]\n",
      "[160.93, 290.81, 693.57, 1864.47]\n",
      "[191.66, -19.87, 756.85, 2049.47]\n",
      "[151.7, -54.29, 756.47, 2088.73]\n",
      "[157.55, 16.38, 792.04, 2016.9]\n",
      "[171.15, 3.48, 798.22, 2078.97]\n",
      "[182.78, 111.61, 724.75, 1897.97]\n",
      "[184.34, 76.23, 709.9, 1976.6]\n",
      "[194.17, -12.83, 709.7, 2113.05]\n",
      "[350.48, -25.34, 546.35, 1956.47]\n",
      "[196.08, -56.5, 703.98, 2089.99]\n",
      "[181.38, 197.57, 733.91, 1813.89]\n",
      "[143.89, -1.03, 814.78, 2109.61]\n",
      "[174.66, -0.56, 778.4, 2045.82]\n",
      "[147.33, -44.7, 816.83, 2096.47]\n",
      "[195.02, 31.76, 751.64, 2059.88]\n",
      "[166.01, -43.0, 797.55, 2099.59]\n",
      "[197.28, -1.69, 721.55, 2050.66]\n",
      "[257.25, 43.54, 684.49, 1993.59]\n",
      "[165.7, 68.62, 817.03, 1973.51]\n",
      "[185.04, 15.28, 730.93, 2015.92]\n",
      "[190.34, 158.9, 668.45, 1720.88]\n",
      "[150.83, 0.72, 823.69, 2053.08]\n",
      "[85.75, 0.4, 910.32, 2200.31]\n",
      "[306.56, -51.54, 634.61, 2100.03]\n",
      "[235.71, 230.24, 647.6, 1684.62]\n",
      "[142.61, 21.95, 780.73, 2022.7]\n",
      "[192.46, 4.57, 752.69, 2032.27]\n",
      "[180.15, -51.21, 756.17, 2122.49]\n",
      "[186.81, 16.11, 758.02, 2012.36]\n",
      "[220.01, 108.22, 702.37, 1864.66]\n",
      "[152.68, 5.3, 733.76, 2042.59]\n",
      "[144.66, -219.05, 862.31, 2303.9]\n",
      "[212.43, 58.96, 732.99, 1958.58]\n",
      "[197.5, -8.72, 709.04, 2101.83]\n",
      "[202.5, 9.26, 714.34, 2053.56]\n",
      "[144.63, -22.11, 815.85, 1995.38]\n",
      "[225.57, 99.81, 701.13, 2040.71]\n",
      "[193.52, 23.69, 753.59, 2034.33]\n",
      "[198.45, 49.22, 773.64, 1934.19]\n",
      "[166.02, 27.11, 776.51, 1961.81]\n",
      "[258.52, 142.53, 672.39, 1865.19]\n",
      "[134.08, 7.25, 769.52, 2078.11]\n",
      "[238.22, 138.98, 704.38, 2000.23]\n",
      "[168.47, 13.46, 767.34, 2044.54]\n",
      "[169.85, 1.56, 747.4, 2058.47]\n",
      "Epoch: [2]  [6/7]  eta: 0:03:15  lr: 0.001000  loss: 8.6753 (8.6815)  loss_classifier: 0.0611 (0.0632)  loss_box_reg: 0.0590 (0.0625)  loss_keypoint: 7.9369 (7.9402)  loss_objectness: 0.5976 (0.5907)  loss_rpn_box_reg: 0.0209 (0.0250)  time: 195.9164  data: 0.7788\n",
      "Epoch: [2] Total time: 0:22:51 (195.9168 s / it)\n",
      "[167.33, -0.29, 763.0, 2049.1]\n",
      "[127.57, -7.18, 809.14, 2067.02]\n",
      "[166.02, 27.11, 776.51, 1961.81]\n",
      "[150.83, 0.72, 823.69, 2053.08]\n",
      "[194.17, -12.83, 709.7, 2113.05]\n",
      "[180.15, -51.21, 756.17, 2122.49]\n",
      "[238.22, 138.98, 704.38, 2000.23]\n",
      "[85.75, 0.4, 910.32, 2200.31]\n",
      "[191.24, 9.85, 705.15, 2061.1]\n",
      "[157.55, 16.38, 792.04, 2016.9]\n",
      "Epoch: [3]  [0/7]  eta: 0:23:17  lr: 0.001000  loss: 8.4829 (8.4829)  loss_classifier: 0.0834 (0.0834)  loss_box_reg: 0.0755 (0.0755)  loss_keypoint: 7.7991 (7.7991)  loss_objectness: 0.4799 (0.4799)  loss_rpn_box_reg: 0.0449 (0.0449)  time: 199.5816  data: 0.7985\n",
      "[143.89, -1.03, 814.78, 2109.61]\n",
      "[152.68, 5.3, 733.76, 2042.59]\n",
      "[232.28, 213.96, 660.17, 1714.34]\n",
      "[197.28, -1.69, 721.55, 2050.66]\n",
      "[144.66, -219.05, 862.31, 2303.9]\n",
      "[173.3, 18.4, 740.54, 2020.4]\n",
      "[147.33, -44.7, 816.83, 2096.47]\n",
      "[190.34, 158.9, 668.45, 1720.88]\n",
      "[182.78, 111.61, 724.75, 1897.97]\n",
      "[160.93, 290.81, 693.57, 1864.47]\n",
      "[165.7, 68.62, 817.03, 1973.51]\n",
      "[134.08, 7.25, 769.52, 2078.11]\n",
      "[306.56, -51.54, 634.61, 2100.03]\n",
      "[258.52, 142.53, 672.39, 1865.19]\n",
      "[187.41, -5.43, 719.19, 2062.01]\n",
      "[194.26, 27.37, 713.76, 1986.82]\n",
      "[171.15, 3.48, 798.22, 2078.97]\n",
      "[192.46, 4.57, 752.69, 2032.27]\n",
      "[168.72, -26.5, 774.65, 2057.63]\n",
      "[181.38, 197.57, 733.91, 1813.89]\n",
      "[151.7, -54.29, 756.47, 2088.73]\n",
      "[239.31, 109.32, 672.63, 1904.35]\n",
      "[168.47, 13.46, 767.34, 2044.54]\n",
      "[169.85, 1.56, 747.4, 2058.47]\n",
      "[193.15, -15.13, 675.13, 2062.17]\n",
      "[212.43, 58.96, 732.99, 1958.58]\n",
      "[225.57, 99.81, 701.13, 2040.71]\n",
      "[197.5, -8.72, 709.04, 2101.83]\n",
      "[184.34, 76.23, 709.9, 1976.6]\n",
      "[350.48, -25.34, 546.35, 1956.47]\n",
      "[193.51, 16.0, 756.04, 1989.76]\n",
      "[257.25, 43.54, 684.49, 1993.59]\n",
      "[181.46, 44.23, 754.87, 1997.44]\n",
      "[146.98, 18.71, 796.27, 2004.04]\n",
      "[174.66, -0.56, 778.4, 2045.82]\n",
      "[214.96, 54.33, 700.32, 1946.17]\n",
      "[186.81, 16.11, 758.02, 2012.36]\n",
      "[193.52, 23.69, 753.59, 2034.33]\n",
      "[170.42, 9.5, 782.99, 2070.8]\n",
      "[166.01, -43.0, 797.55, 2099.59]\n",
      "[185.04, 15.28, 730.93, 2015.92]\n",
      "[191.66, -19.87, 756.85, 2049.47]\n",
      "[278.3, 189.58, 594.08, 1730.92]\n",
      "[142.61, 21.95, 780.73, 2022.7]\n",
      "[196.08, -56.5, 703.98, 2089.99]\n",
      "[246.02, 76.79, 645.68, 1796.36]\n",
      "[202.5, 9.26, 714.34, 2053.56]\n",
      "[288.52, 59.7, 588.82, 2027.28]\n",
      "[195.43, 76.37, 715.05, 1961.72]\n",
      "[198.45, 49.22, 773.64, 1934.19]\n",
      "[187.77, -26.23, 698.18, 2001.71]\n",
      "[144.63, -22.11, 815.85, 1995.38]\n",
      "[155.8, -0.73, 818.09, 2048.1]\n",
      "[235.71, 230.24, 647.6, 1684.62]\n",
      "[183.88, 22.16, 773.79, 2001.75]\n",
      "[220.01, 108.22, 702.37, 1864.66]\n",
      "[114.9, -30.6, 857.57, 2082.25]\n",
      "[243.17, 134.41, 664.01, 1753.4]\n",
      "[195.02, 31.76, 751.64, 2059.88]\n",
      "[219.64, 124.89, 770.69, 1870.89]\n",
      "Epoch: [3]  [6/7]  eta: 0:03:20  lr: 0.001000  loss: 8.0471 (8.0724)  loss_classifier: 0.0451 (0.0517)  loss_box_reg: 0.0691 (0.0705)  loss_keypoint: 7.5682 (7.5811)  loss_objectness: 0.3464 (0.3427)  loss_rpn_box_reg: 0.0231 (0.0262)  time: 200.1098  data: 0.7818\n",
      "Epoch: [3] Total time: 0:23:20 (200.1100 s / it)\n",
      "[193.52, 23.69, 753.59, 2034.33]\n",
      "[190.34, 158.9, 668.45, 1720.88]\n",
      "[232.28, 213.96, 660.17, 1714.34]\n",
      "[168.47, 13.46, 767.34, 2044.54]\n",
      "[235.71, 230.24, 647.6, 1684.62]\n",
      "[127.57, -7.18, 809.14, 2067.02]\n",
      "[238.22, 138.98, 704.38, 2000.23]\n",
      "[225.57, 99.81, 701.13, 2040.71]\n",
      "[167.33, -0.29, 763.0, 2049.1]\n",
      "[192.46, 4.57, 752.69, 2032.27]\n",
      "Epoch: [4]  [0/7]  eta: 0:24:10  lr: 0.001000  loss: 7.4339 (7.4339)  loss_classifier: 0.0271 (0.0271)  loss_box_reg: 0.0763 (0.0763)  loss_keypoint: 7.1530 (7.1530)  loss_objectness: 0.1567 (0.1567)  loss_rpn_box_reg: 0.0209 (0.0209)  time: 207.1521  data: 0.8035\n",
      "[181.38, 197.57, 733.91, 1813.89]\n",
      "[187.77, -26.23, 698.18, 2001.71]\n",
      "[195.02, 31.76, 751.64, 2059.88]\n",
      "[194.26, 27.37, 713.76, 1986.82]\n",
      "[191.66, -19.87, 756.85, 2049.47]\n",
      "[196.08, -56.5, 703.98, 2089.99]\n",
      "[157.55, 16.38, 792.04, 2016.9]\n",
      "[214.96, 54.33, 700.32, 1946.17]\n",
      "[150.83, 0.72, 823.69, 2053.08]\n",
      "[243.17, 134.41, 664.01, 1753.4]\n",
      "[350.48, -25.34, 546.35, 1956.47]\n",
      "[169.85, 1.56, 747.4, 2058.47]\n",
      "[239.31, 109.32, 672.63, 1904.35]\n",
      "[220.01, 108.22, 702.37, 1864.66]\n",
      "[219.64, 124.89, 770.69, 1870.89]\n",
      "[193.51, 16.0, 756.04, 1989.76]\n",
      "[165.7, 68.62, 817.03, 1973.51]\n",
      "[185.04, 15.28, 730.93, 2015.92]\n",
      "[170.42, 9.5, 782.99, 2070.8]\n",
      "[166.01, -43.0, 797.55, 2099.59]\n",
      "[288.52, 59.7, 588.82, 2027.28]\n",
      "[160.93, 290.81, 693.57, 1864.47]\n",
      "[184.34, 76.23, 709.9, 1976.6]\n",
      "[85.75, 0.4, 910.32, 2200.31]\n",
      "[146.98, 18.71, 796.27, 2004.04]\n",
      "[183.88, 22.16, 773.79, 2001.75]\n",
      "[194.17, -12.83, 709.7, 2113.05]\n",
      "[197.28, -1.69, 721.55, 2050.66]\n",
      "[198.45, 49.22, 773.64, 1934.19]\n",
      "[191.24, 9.85, 705.15, 2061.1]\n",
      "[151.7, -54.29, 756.47, 2088.73]\n",
      "[195.43, 76.37, 715.05, 1961.72]\n",
      "[147.33, -44.7, 816.83, 2096.47]\n",
      "[181.46, 44.23, 754.87, 1997.44]\n",
      "[173.3, 18.4, 740.54, 2020.4]\n",
      "[143.89, -1.03, 814.78, 2109.61]\n",
      "[202.5, 9.26, 714.34, 2053.56]\n",
      "[257.25, 43.54, 684.49, 1993.59]\n",
      "[168.72, -26.5, 774.65, 2057.63]\n",
      "[134.08, 7.25, 769.52, 2078.11]\n",
      "[144.63, -22.11, 815.85, 1995.38]\n",
      "[212.43, 58.96, 732.99, 1958.58]\n",
      "[258.52, 142.53, 672.39, 1865.19]\n",
      "[306.56, -51.54, 634.61, 2100.03]\n",
      "[187.41, -5.43, 719.19, 2062.01]\n",
      "[114.9, -30.6, 857.57, 2082.25]\n",
      "[142.61, 21.95, 780.73, 2022.7]\n",
      "[174.66, -0.56, 778.4, 2045.82]\n",
      "[144.66, -219.05, 862.31, 2303.9]\n",
      "[246.02, 76.79, 645.68, 1796.36]\n",
      "[155.8, -0.73, 818.09, 2048.1]\n",
      "[182.78, 111.61, 724.75, 1897.97]\n",
      "[193.15, -15.13, 675.13, 2062.17]\n",
      "[197.5, -8.72, 709.04, 2101.83]\n",
      "[166.02, 27.11, 776.51, 1961.81]\n",
      "[186.81, 16.11, 758.02, 2012.36]\n",
      "[171.15, 3.48, 798.22, 2078.97]\n",
      "[180.15, -51.21, 756.17, 2122.49]\n",
      "[152.68, 5.3, 733.76, 2042.59]\n",
      "[278.3, 189.58, 594.08, 1730.92]\n",
      "Epoch: [4]  [6/7]  eta: 0:04:36  lr: 0.001000  loss: 6.7742 (6.7893)  loss_classifier: 0.0312 (0.0321)  loss_box_reg: 0.0677 (0.0675)  loss_keypoint: 6.5665 (6.5856)  loss_objectness: 0.0541 (0.0737)  loss_rpn_box_reg: 0.0258 (0.0305)  time: 276.2443  data: 0.7897\n",
      "Epoch: [4] Total time: 0:32:13 (276.2447 s / it)\n",
      "[306.56, -51.54, 634.61, 2100.03]\n",
      "[182.78, 111.61, 724.75, 1897.97]\n",
      "[165.7, 68.62, 817.03, 1973.51]\n",
      "[258.52, 142.53, 672.39, 1865.19]\n",
      "[134.08, 7.25, 769.52, 2078.11]\n",
      "[350.48, -25.34, 546.35, 1956.47]\n",
      "[151.7, -54.29, 756.47, 2088.73]\n",
      "[195.43, 76.37, 715.05, 1961.72]\n",
      "[144.63, -22.11, 815.85, 1995.38]\n",
      "[147.33, -44.7, 816.83, 2096.47]\n",
      "Epoch: [5]  [0/7]  eta: 0:57:58  lr: 0.000300  loss: 6.2195 (6.2195)  loss_classifier: 0.0393 (0.0393)  loss_box_reg: 0.0787 (0.0787)  loss_keypoint: 6.0503 (6.0503)  loss_objectness: 0.0317 (0.0317)  loss_rpn_box_reg: 0.0195 (0.0195)  time: 496.9866  data: 0.7965\n",
      "[114.9, -30.6, 857.57, 2082.25]\n",
      "[168.72, -26.5, 774.65, 2057.63]\n",
      "[288.52, 59.7, 588.82, 2027.28]\n",
      "[198.45, 49.22, 773.64, 1934.19]\n",
      "[278.3, 189.58, 594.08, 1730.92]\n",
      "[194.17, -12.83, 709.7, 2113.05]\n",
      "[171.15, 3.48, 798.22, 2078.97]\n",
      "[194.26, 27.37, 713.76, 1986.82]\n",
      "[169.85, 1.56, 747.4, 2058.47]\n",
      "[155.8, -0.73, 818.09, 2048.1]\n",
      "[220.01, 108.22, 702.37, 1864.66]\n",
      "[193.15, -15.13, 675.13, 2062.17]\n",
      "[192.46, 4.57, 752.69, 2032.27]\n",
      "[238.22, 138.98, 704.38, 2000.23]\n",
      "[143.89, -1.03, 814.78, 2109.61]\n",
      "[166.02, 27.11, 776.51, 1961.81]\n",
      "[183.88, 22.16, 773.79, 2001.75]\n",
      "[166.01, -43.0, 797.55, 2099.59]\n",
      "[197.28, -1.69, 721.55, 2050.66]\n",
      "[160.93, 290.81, 693.57, 1864.47]\n",
      "[127.57, -7.18, 809.14, 2067.02]\n",
      "[180.15, -51.21, 756.17, 2122.49]\n",
      "[191.24, 9.85, 705.15, 2061.1]\n",
      "[187.41, -5.43, 719.19, 2062.01]\n",
      "[167.33, -0.29, 763.0, 2049.1]\n",
      "[185.04, 15.28, 730.93, 2015.92]\n",
      "[157.55, 16.38, 792.04, 2016.9]\n",
      "[195.02, 31.76, 751.64, 2059.88]\n",
      "[190.34, 158.9, 668.45, 1720.88]\n",
      "[146.98, 18.71, 796.27, 2004.04]\n",
      "[202.5, 9.26, 714.34, 2053.56]\n",
      "[243.17, 134.41, 664.01, 1753.4]\n",
      "[150.83, 0.72, 823.69, 2053.08]\n",
      "[152.68, 5.3, 733.76, 2042.59]\n",
      "[142.61, 21.95, 780.73, 2022.7]\n",
      "[168.47, 13.46, 767.34, 2044.54]\n",
      "[235.71, 230.24, 647.6, 1684.62]\n",
      "[144.66, -219.05, 862.31, 2303.9]\n",
      "[219.64, 124.89, 770.69, 1870.89]\n",
      "[191.66, -19.87, 756.85, 2049.47]\n",
      "[186.81, 16.11, 758.02, 2012.36]\n",
      "[184.34, 76.23, 709.9, 1976.6]\n",
      "[181.38, 197.57, 733.91, 1813.89]\n",
      "[212.43, 58.96, 732.99, 1958.58]\n",
      "[257.25, 43.54, 684.49, 1993.59]\n",
      "[173.3, 18.4, 740.54, 2020.4]\n",
      "[181.46, 44.23, 754.87, 1997.44]\n",
      "[239.31, 109.32, 672.63, 1904.35]\n",
      "[193.52, 23.69, 753.59, 2034.33]\n",
      "[225.57, 99.81, 701.13, 2040.71]\n",
      "[85.75, 0.4, 910.32, 2200.31]\n",
      "[174.66, -0.56, 778.4, 2045.82]\n",
      "[214.96, 54.33, 700.32, 1946.17]\n",
      "[197.5, -8.72, 709.04, 2101.83]\n",
      "[170.42, 9.5, 782.99, 2070.8]\n",
      "[246.02, 76.79, 645.68, 1796.36]\n",
      "[232.28, 213.96, 660.17, 1714.34]\n",
      "[196.08, -56.5, 703.98, 2089.99]\n",
      "[187.77, -26.23, 698.18, 2001.71]\n",
      "[193.51, 16.0, 756.04, 1989.76]\n",
      "Epoch: [5]  [6/7]  eta: 0:10:07  lr: 0.000300  loss: 5.7455 (5.8377)  loss_classifier: 0.0339 (0.0342)  loss_box_reg: 0.0572 (0.0599)  loss_keypoint: 5.6124 (5.7021)  loss_objectness: 0.0228 (0.0229)  loss_rpn_box_reg: 0.0186 (0.0186)  time: 607.0645  data: 0.7900\n",
      "Epoch: [5] Total time: 1:10:49 (607.0646 s / it)\n",
      "[169.85, 1.56, 747.4, 2058.47]\n",
      "[142.61, 21.95, 780.73, 2022.7]\n",
      "[212.43, 58.96, 732.99, 1958.58]\n",
      "[190.34, 158.9, 668.45, 1720.88]\n",
      "[181.46, 44.23, 754.87, 1997.44]\n",
      "[150.83, 0.72, 823.69, 2053.08]\n",
      "[171.15, 3.48, 798.22, 2078.97]\n",
      "[174.66, -0.56, 778.4, 2045.82]\n",
      "[239.31, 109.32, 672.63, 1904.35]\n",
      "[127.57, -7.18, 809.14, 2067.02]\n",
      "Epoch: [6]  [0/7]  eta: 1:13:48  lr: 0.000300  loss: 5.5543 (5.5543)  loss_classifier: 0.0294 (0.0294)  loss_box_reg: 0.0537 (0.0537)  loss_keypoint: 5.4354 (5.4354)  loss_objectness: 0.0151 (0.0151)  loss_rpn_box_reg: 0.0208 (0.0208)  time: 632.6078  data: 0.7975\n",
      "[165.7, 68.62, 817.03, 1973.51]\n",
      "[258.52, 142.53, 672.39, 1865.19]\n",
      "[195.02, 31.76, 751.64, 2059.88]\n",
      "[192.46, 4.57, 752.69, 2032.27]\n",
      "[238.22, 138.98, 704.38, 2000.23]\n",
      "[168.47, 13.46, 767.34, 2044.54]\n",
      "[185.04, 15.28, 730.93, 2015.92]\n",
      "[246.02, 76.79, 645.68, 1796.36]\n",
      "[160.93, 290.81, 693.57, 1864.47]\n",
      "[184.34, 76.23, 709.9, 1976.6]\n",
      "[257.25, 43.54, 684.49, 1993.59]\n",
      "[151.7, -54.29, 756.47, 2088.73]\n",
      "[143.89, -1.03, 814.78, 2109.61]\n",
      "[193.51, 16.0, 756.04, 1989.76]\n",
      "[147.33, -44.7, 816.83, 2096.47]\n",
      "[191.24, 9.85, 705.15, 2061.1]\n",
      "[194.26, 27.37, 713.76, 1986.82]\n",
      "[198.45, 49.22, 773.64, 1934.19]\n",
      "[243.17, 134.41, 664.01, 1753.4]\n",
      "[197.5, -8.72, 709.04, 2101.83]\n",
      "[225.57, 99.81, 701.13, 2040.71]\n",
      "[157.55, 16.38, 792.04, 2016.9]\n",
      "[134.08, 7.25, 769.52, 2078.11]\n",
      "[196.08, -56.5, 703.98, 2089.99]\n",
      "[114.9, -30.6, 857.57, 2082.25]\n",
      "[278.3, 189.58, 594.08, 1730.92]\n",
      "[187.77, -26.23, 698.18, 2001.71]\n",
      "[194.17, -12.83, 709.7, 2113.05]\n",
      "[195.43, 76.37, 715.05, 1961.72]\n",
      "[170.42, 9.5, 782.99, 2070.8]\n",
      "[235.71, 230.24, 647.6, 1684.62]\n",
      "[166.01, -43.0, 797.55, 2099.59]\n",
      "[193.15, -15.13, 675.13, 2062.17]\n",
      "[183.88, 22.16, 773.79, 2001.75]\n",
      "[191.66, -19.87, 756.85, 2049.47]\n",
      "[146.98, 18.71, 796.27, 2004.04]\n",
      "[180.15, -51.21, 756.17, 2122.49]\n",
      "[85.75, 0.4, 910.32, 2200.31]\n",
      "[144.63, -22.11, 815.85, 1995.38]\n",
      "[152.68, 5.3, 733.76, 2042.59]\n",
      "[173.3, 18.4, 740.54, 2020.4]\n",
      "[197.28, -1.69, 721.55, 2050.66]\n",
      "[187.41, -5.43, 719.19, 2062.01]\n",
      "[166.02, 27.11, 776.51, 1961.81]\n",
      "[214.96, 54.33, 700.32, 1946.17]\n",
      "[155.8, -0.73, 818.09, 2048.1]\n",
      "[181.38, 197.57, 733.91, 1813.89]\n",
      "[144.66, -219.05, 862.31, 2303.9]\n",
      "[167.33, -0.29, 763.0, 2049.1]\n",
      "[219.64, 124.89, 770.69, 1870.89]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[75], line 25\u001B[0m\n\u001B[0;32m     22\u001B[0m num_epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m30\u001B[39m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[1;32m---> 25\u001B[0m     \u001B[43mtrain_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_loader_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprint_freq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     26\u001B[0m     lr_scheduler\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     27\u001B[0m     \u001B[38;5;66;03m# evaluate(model, data_loader_test, device)\u001B[39;00m\n\u001B[0;32m     28\u001B[0m \n\u001B[0;32m     29\u001B[0m \n\u001B[0;32m     30\u001B[0m \u001B[38;5;66;03m# Save model weights after training\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\skeleton\\engine.py:47\u001B[0m, in \u001B[0;36mtrain_one_epoch\u001B[1;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001B[0m\n\u001B[0;32m     44\u001B[0m     sys\u001B[38;5;241m.\u001B[39mexit(\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     46\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 47\u001B[0m \u001B[43mlosses\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     48\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m lr_scheduler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\skeleton\\venv\\lib\\site-packages\\torch\\_tensor.py:488\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    478\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    479\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    480\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    481\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    486\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    487\u001B[0m     )\n\u001B[1;32m--> 488\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    490\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\skeleton\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('training with cpu')\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "KEYPOINTS_FOLDER_TRAIN = r\"C:\\Users\\Administrator\\Downloads\\sk0413\\annotations\\person_keypoints_default.json\"\n",
    "KEYPOINTS_FOLDER_TEST = r\"C:\\Users\\Administrator\\Downloads\\sk0413\\annotations\\person_keypoints_default.json\"\n",
    "\n",
    "dataset_train = CocoDataset(annFile=KEYPOINTS_FOLDER_TRAIN,root=img_path,transform=None, demo=False)\n",
    "dataset_test = CocoDataset(annFile=KEYPOINTS_FOLDER_TEST,root=img_path, transform=None, demo=False)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=10, shuffle=True, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model = get_model(num_keypoints = 137)\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=10)\n",
    "    lr_scheduler.step()\n",
    "    # evaluate(model, data_loader_test, device)\n",
    "\n",
    "\n",
    "# Save model weights after training\n",
    "torch.save(model.state_dict(), r'C:\\Users\\Administrator\\PycharmProjects\\skeleton\\out\\keypointsrcnn_weights.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "iterator = iter(data_loader_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[288.52, 59.7, 588.82, 2027.28]\n",
      "Predictions: \n",
      " [{'boxes': tensor([[ 319.0503,  271.8174,  854.3345, 1844.7832],\n",
      "        [ 156.6725,    0.0000, 1031.1033, 2048.0000],\n",
      "        [ 113.9429,    0.0000,  909.5860,  955.5572],\n",
      "        [  10.0822,  366.3880, 1099.0996, 1026.9315],\n",
      "        [  46.9870,  223.0565, 1097.8053,  775.0717],\n",
      "        [  72.3679, 1241.6851, 1120.0000, 1832.3147],\n",
      "        [ 111.5035,  984.4159, 1120.0000, 1526.1805],\n",
      "        [  76.9502,  709.2543, 1064.4347, 1270.4849],\n",
      "        [ 236.2540,  940.5580,  915.4507, 2048.0000],\n",
      "        [ 712.0825,  647.0903, 1120.0000, 2048.0000],\n",
      "        [   0.0000,    0.0000,  341.9474, 1718.1290],\n",
      "        [   0.0000,  705.5343,  480.8356, 2048.0000],\n",
      "        [ 569.0739,    0.0000, 1093.0115, 1228.1108]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'scores': tensor([0.8098, 0.7950, 0.6553, 0.6405, 0.5450, 0.5408, 0.4793, 0.4264, 0.3798,\n",
      "        0.2503, 0.2458, 0.2041, 0.1383]), 'keypoints': tensor([[[4.6595e+02, 2.7719e+02, 1.0000e+00],\n",
      "         [4.6287e+02, 2.7719e+02, 1.0000e+00],\n",
      "         [5.1978e+02, 2.7719e+02, 1.0000e+00],\n",
      "         ...,\n",
      "         [4.6287e+02, 5.2297e+02, 1.0000e+00],\n",
      "         [5.9669e+02, 1.7626e+03, 1.0000e+00],\n",
      "         [7.3051e+02, 1.3694e+03, 1.0000e+00]],\n",
      "\n",
      "        [[4.5711e+02, 5.3773e+00, 1.0000e+00],\n",
      "         [4.5404e+02, 1.7592e+02, 1.0000e+00],\n",
      "         [4.8478e+02, 5.3773e+00, 1.0000e+00],\n",
      "         ...,\n",
      "         [6.0926e+02, 1.8736e+03, 1.0000e+00],\n",
      "         [4.8631e+02, 1.5034e+03, 1.0000e+00],\n",
      "         [7.6601e+02, 1.2806e+03, 1.0000e+00]],\n",
      "\n",
      "        [[3.8658e+02, 2.3044e+00, 1.0000e+00],\n",
      "         [4.7106e+02, 1.8819e+02, 1.0000e+00],\n",
      "         [4.6799e+02, 2.3044e+00, 1.0000e+00],\n",
      "         ...,\n",
      "         [5.2636e+02, 8.7490e+02, 1.0000e+00],\n",
      "         [5.8319e+02, 9.0716e+02, 1.0000e+00],\n",
      "         [6.6767e+02, 8.7029e+02, 1.0000e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[4.3702e+01, 5.3740e+00, 1.0000e+00],\n",
      "         [2.6298e+02, 2.7561e+02, 1.0000e+00],\n",
      "         [2.8751e+02, 5.3740e+00, 1.0000e+00],\n",
      "         ...,\n",
      "         [2.4918e+02, 1.1969e+03, 1.0000e+00],\n",
      "         [1.7711e+02, 1.5945e+03, 1.0000e+00],\n",
      "         [2.1391e+02, 1.3182e+03, 1.0000e+00]],\n",
      "\n",
      "        [[3.8636e+02, 7.0937e+02, 1.0000e+00],\n",
      "         [4.2169e+02, 1.0642e+03, 1.0000e+00],\n",
      "         [4.0479e+02, 7.0937e+02, 1.0000e+00],\n",
      "         ...,\n",
      "         [3.5256e+02, 1.9320e+03, 1.0000e+00],\n",
      "         [3.1723e+02, 1.9412e+03, 1.0000e+00],\n",
      "         [1.2827e+02, 1.7370e+03, 1.0000e+00]],\n",
      "\n",
      "        [[9.7240e+02, 3.8378e+00, 1.0000e+00],\n",
      "         [5.9750e+02, 7.2228e+02, 1.0000e+00],\n",
      "         [6.3437e+02, 1.1644e+03, 1.0000e+00],\n",
      "         ...,\n",
      "         [6.9122e+02, 1.2243e+03, 1.0000e+00],\n",
      "         [6.9276e+02, 1.1230e+03, 1.0000e+00],\n",
      "         [7.4654e+02, 1.0308e+03, 1.0000e+00]]]), 'keypoints_scores': tensor([[4.7933, 8.5341, 8.4687,  ..., 5.0343, 5.4502, 3.3229],\n",
      "        [4.5352, 6.4015, 5.1744,  ..., 4.2760, 4.3775, 4.0604],\n",
      "        [2.9699, 5.1963, 3.2644,  ..., 4.4815, 4.8048, 3.8991],\n",
      "        ...,\n",
      "        [1.5923, 2.6729, 1.7082,  ..., 2.6491, 2.7512, 2.7582],\n",
      "        [2.2135, 3.5350, 2.4415,  ..., 3.2209, 2.2775, 1.7149],\n",
      "        [1.7112, 3.6223, 2.6693,  ..., 3.4417, 2.9779, 3.6010]])}]\n"
     ]
    }
   ],
   "source": [
    "images, targets = next(iterator)\n",
    "images = list(image.to(device) for image in images)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    output = model(images)\n",
    "\n",
    "print(\"Predictions: \\n\", output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no keypoint\n"
     ]
    }
   ],
   "source": [
    "image = (images[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "scores = output[0]['scores'].detach().cpu().numpy()\n",
    "\n",
    "high_scores_idxs = np.where(scores > 0.7)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "# Below, in output[0]['keypoints'][high_scores_idxs][post_nms_idxs] and output[0]['boxes'][high_scores_idxs][post_nms_idxs]\n",
    "# Firstly, we choose only those objects, which have score above predefined threshold. This is done with choosing elements with [high_scores_idxs] indexes\n",
    "# Secondly, we choose only those objects, which are left after NMS is applied. This is done with choosing elements with [post_nms_idxs] indexes\n",
    "\n",
    "keypoints = []\n",
    "for kps in output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "\n",
    "bboxes = []\n",
    "for bbox in output[0]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    bboxes.append(list(map(int, bbox.tolist())))\n",
    "\n",
    "visualize(image, bboxes, keypoints)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}